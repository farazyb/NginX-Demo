name: Deploy (Docker Hub → AWS EKS)

on:
  push:
    branches: [ "main" ]
  workflow_dispatch: {}

permissions:
  contents: read
  id-token: write   # required for GitHub→AWS OIDC

env:
  AWS_REGION: eu-north-1
  EKS_CLUSTER_NAME: amazing-synth-shark
  NAMESPACE: default
  MANIFEST: resources.yaml
  DOCKERHUB_USER: ${{ secrets.DOCKERHUB_USERNAME }}
  IMAGE_NAME: nginx
  IMAGE_TAG: ${{ github.sha }}

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # Build & push → Docker Hub
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push image
        uses: docker/build-push-action@v6
        with:
          context: .
          push: true
          tags: |
            docker.io/${{ env.DOCKERHUB_USER }}/${{ env.IMAGE_NAME }}:latest
            docker.io/${{ env.DOCKERHUB_USER }}/${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      # OIDC → assume role (no static keys)
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::644701780664:role/GitHubActionsEKSRole
          role-session-name: GitHub_to_AWS_via_OIDC
          aws-region: ${{ env.AWS_REGION }}

      # kubectl (stable)
      - name: Install kubectl
        run: |
          KUBECTL_VERSION=$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)
          curl -LO "https://dl.k8s.io/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/
          kubectl version --client

      # kubeconfig for EKS (do NOT pass --role-arn; we already assumed it)
      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name "${EKS_CLUSTER_NAME}" --region "${AWS_REGION}"
          kubectl config current-context

      # (Optional) quick identity checks while stabilizing
      - name: Who am I (AWS)?
        run: aws sts get-caller-identity
      - name: Who am I (K8s)?
        run: kubectl auth can-i --list || true

      # Render manifest (${IMAGE} in your resources.yaml)
      - name: Render manifest
        run: |
          command -v envsubst >/dev/null 2>&1 || { sudo apt-get update && sudo apt-get install -y gettext-base; }
          export IMAGE="docker.io/${DOCKERHUB_USER}/${IMAGE_NAME}:${IMAGE_TAG}"
          envsubst < "${MANIFEST}" > out.yaml
          echo "Using image: ${IMAGE}"

      # Ensure namespace exists (idempotent)
      - name: Ensure namespace
        run: |
          kubectl create namespace "${NAMESPACE}" --dry-run=client -o yaml | kubectl apply -f - --validate=false

      # Deploy and wait for rollout
      - name: Deploy to EKS
        run: |
          kubectl apply -n "${NAMESPACE}" -f out.yaml --validate=false
          kubectl rollout status deploy/nginx -n "${NAMESPACE}" --timeout=180s
          kubectl get svc -n "${NAMESPACE}" -o wide
